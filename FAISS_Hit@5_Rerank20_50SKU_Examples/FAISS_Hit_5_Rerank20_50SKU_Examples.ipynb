{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# FAISS + Confidence-Based Reranking for SKU Matching\n"
      ],
      "metadata": {
        "id": "VNg_a6G9rjEe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook implements a two-step retrieval pipeline using FAISS (top-20 retrieval) followed by confidence-based reranking to evaluate accuracy and false positives across confidence thresholds.\n"
      ],
      "metadata": {
        "id": "AJYdSUHtP3Gh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Load Input Dataset"
      ],
      "metadata": {
        "id": "WsVyFGiHP3SY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 0. Load data\n",
        "csv_file = '/content/drive/MyDrive/HackWeekC1/HackWeekProductsData.csv'\n",
        "df_source = pd.read_csv(csv_file, encoding='utf-8-sig')\n",
        "print(\"Number of rows:\", len(df_source))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lEWstAaRP3kw",
        "outputId": "5e83ad0a-399b-42ed-dcc9-9b323df1475b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of rows: 4165840\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) Count how many examples per SKU\n",
        "sku_counts = df_source['ProductMasterId'].value_counts()"
      ],
      "metadata": {
        "id": "GWbUAa5aQO6u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2) Print basic stats\n",
        "print(f\"Total distinct SKUs: {sku_counts.shape[0]}\")\n",
        "print(f\"  • Max occurrences: {sku_counts.max()}  (SKU {sku_counts.idxmax()})\")\n",
        "print(f\"  • Min occurrences: {sku_counts.min()}  (example SKUs: {sku_counts[sku_counts==sku_counts.min()].index[:5].tolist()}…)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aens4JrnP9dk",
        "outputId": "8eb4ab15-ce49-4e68-a922-d05c17f37dd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total distinct SKUs: 95016\n",
            "  • Max occurrences: 1168385  (SKU 0)\n",
            "  • Min occurrences: 1  (example SKUs: [90449, 130728, 20962, 122172, 145046]…)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Count Examples Per SKU\n",
        "Count how many times each SKU appears to understand class distribution."
      ],
      "metadata": {
        "id": "yDFdYDuEZFS4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 4) See the long tail:\n",
        "print(\"\\nNumber of SKUs with ≤5 examples:\", (sku_counts <= 5).sum())\n",
        "print(\"Number of SKUs with ≥5 examples:\", (sku_counts >= 5).sum())\n",
        "print(\"Number of SKUs with ≥10 examples:\", (sku_counts >= 10).sum())\n",
        "print(\"Number of SKUs with ≥20 examples:\", (sku_counts >= 20).sum())\n",
        "print(\"Number of SKUs with ≥50 examples:\", (sku_counts >= 50).sum())\n",
        "print(\"Number of SKUs with ≥100 examples:\", (sku_counts >= 100).sum())\n",
        "print(\"Number of SKUs with ≥1000 examples:\", (sku_counts >= 1000).sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33C6_pLxQC91",
        "outputId": "9c4e0405-dca6-4eab-b10f-dd0ffe88097b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Number of SKUs with ≤5 examples: 50664\n",
            "Number of SKUs with ≥5 examples: 48507\n",
            "Number of SKUs with ≥10 examples: 34473\n",
            "Number of SKUs with ≥20 examples: 23350\n",
            "Number of SKUs with ≥50 examples: 12253\n",
            "Number of SKUs with ≥100 examples: 6760\n",
            "Number of SKUs with ≥1000 examples: 168\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Train FAISS + Reranker Model\n",
        "Run FAISS-based retrieval on queries, then apply a logistic reranker to compute top-1 match confidence scores and evaluate performance.\n"
      ],
      "metadata": {
        "id": "Slu8ES0TZOMG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# challenge1_faiss_rerank_top5_sampling.py\n",
        "# Two-stage retrieval + rerank pipeline with top-5 evaluation\n",
        "# Sampling: fixed examples per SKU + overall max rows\n",
        "# Usage: Run in Google Colab. Upload your CSV (e.g., 'HackWeekProductsData.csv') to working directory.\n",
        "\n",
        "# 0. Install dependencies\n",
        "!pip install --quiet faiss-cpu sentence-transformers scikit-learn pandas numpy rapidfuzz\n",
        "\n",
        "# 1. Imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import faiss\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from rapidfuzz import fuzz\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "# 2. Load full data then sample by SKU\n",
        "csv_file = '/content/drive/MyDrive/HackWeekC1/HackWeekProductsData.csv'\n",
        "# sampling params\n",
        "min_examples = 50     # examples per SKU\n",
        "max_rows     = 600000  # total rows cap\n",
        "print(f\"Loading full data from {csv_file}...\")\n",
        "df_full = pd.read_csv(csv_file, encoding='utf-8-sig')\n",
        "print(f\"→ Total rows read: {len(df_full)}\")\n",
        "\n",
        "# 3. Filter to 'Map' rows\n",
        "if 'MappingModeName' in df_full.columns:\n",
        "    before = len(df_full)\n",
        "    df_full = df_full[df_full['MappingModeName'].str.strip().str.lower()=='map']\n",
        "    print(f\"Filtered to {len(df_full)} mapped rows (dropped {before-len(df_full)}).\")\n",
        "else:\n",
        "    print(\"No MappingModeName; proceeding with all rows.\")\n",
        "\n",
        "# 4. Select SKUs with >= min_examples occurrences\n",
        "sku_counts = df_full['ProductMasterId'].value_counts()\n",
        "eligible_skus = sku_counts[sku_counts >= min_examples].index\n",
        "print(f\"Found {len(eligible_skus)} SKUs with >= {min_examples} examples.\")\n",
        "\n",
        "# 5. Limit number of SKUs by total cap\n",
        "max_skus = max_rows // min_examples\n",
        "selected_skus = list(sku_counts.loc[eligible_skus].nlargest(max_skus).index)\n",
        "print(f\"Selecting top {len(selected_skus)} SKUs (max cap {max_skus}) => target rows {len(selected_skus)*min_examples}\")\n",
        "\n",
        "# 6. Sample min_examples per SKU\n",
        "df_source = (\n",
        "    df_full[df_full['ProductMasterId'].isin(selected_skus)]\n",
        "    .groupby('ProductMasterId', group_keys=False)\n",
        "    .apply(lambda g: g.sample(n=min_examples, random_state=42))\n",
        "    .reset_index(drop=True)\n",
        ")\n",
        "print(f\"Sampled dataframe: {len(df_source)} rows across {df_source['ProductMasterId'].nunique()} SKUs.\")\n",
        "\n",
        "# 7. Prepare master DataFrame—only selected SKUs\n",
        "source_cols = [\n",
        "    'ServiceAndProductMappingId','SourceMasterBrand','SourceBrand','SourceDescription',\n",
        "    'SourceCategory','SourceSubcategory','SourceSize','SourceUnitOfMeasure',\n",
        "    'CleanBarcode','IsValidBarcode','ProductMasterId'\n",
        "]\n",
        "master_cols = [\n",
        "    'ProductMasterId','MasterBrandName','BrandName','SubBrandName',\n",
        "    'ProductName','CategoryName','SubcategoryFormName','SubcategoryFunctionName',\n",
        "    'ProductSizeOnLabel','CleanBarcode'\n",
        "]\n",
        "missing = set(source_cols+master_cols) - set(df_full.columns)\n",
        "if missing:\n",
        "    raise ValueError(f\"Missing columns: {missing}\")\n",
        "# filter master to only selected SKUs\n",
        "\n",
        "df_master = (\n",
        "    df_full[df_full['ProductMasterId'].isin(selected_skus)]\n",
        "    [master_cols]\n",
        "    .drop_duplicates('ProductMasterId')\n",
        "    .set_index('ProductMasterId')\n",
        ")\n",
        "print(f\"Master SKUs in index: {len(df_master)}\")\n",
        "\n",
        "# 8. Build KB texts\n",
        "kb_ids = list(df_master.index)\n",
        "kb_texts = [\n",
        "    ' '.join(filter(None, [row.MasterBrandName, row.BrandName, row.ProductName,\n",
        "                            row.CategoryName, row.SubcategoryFormName, row.SubcategoryFunctionName,\n",
        "                            str(row.ProductSizeOnLabel)]))\n",
        "    for _, row in df_master.iterrows()\n",
        "]\n",
        "\n",
        "# 9. Normalize queries\n",
        "def normalize_source(r):\n",
        "    parts = [r.SourceMasterBrand, r.SourceBrand, r.SourceDescription,\n",
        "             r.SourceCategory, r.SourceSubcategory, str(r.SourceSize), str(r.SourceUnitOfMeasure)]\n",
        "    return ' '.join(str(p) for p in parts if pd.notna(p) and p)\n",
        "\n",
        "df_source['query_text'] = df_source.apply(normalize_source, axis=1)\n",
        "\n",
        "# 10. Embed\n",
        "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "print(\"Embedding KB...\")\n",
        "kb_emb = embedder.encode(kb_texts, show_progress_bar=True)\n",
        "print(\"Embedding queries...\")\n",
        "query_emb = embedder.encode(df_source['query_text'].tolist(), show_progress_bar=True)\n",
        "\n",
        "# 11. FAISS index\n",
        "faiss.normalize_L2(kb_emb)\n",
        "dim = kb_emb.shape[1]\n",
        "index = faiss.IndexFlatIP(dim)\n",
        "index.add(kb_emb)\n",
        "print(f\"FAISS index built: {index.ntotal} entries, dim={dim}\")\n",
        "\n",
        "# 12. Retrieve top-K\n",
        "faiss.normalize_L2(query_emb)\n",
        "k = 20\n",
        "D, I = index.search(query_emb, k)\n",
        "print(f\"Retrieved top-{k} candidates for {len(df_source)} queries.\")\n",
        "\n",
        "# 13. Parse size feature\n",
        "def parse_size(text):\n",
        "    try:\n",
        "        val, unit = re.search(r\"(\\d+\\.?\\d*)\\s*(ml|l|oz|fl oz)\", str(text).lower()).groups()\n",
        "        unit_map = {'ml':1,'l':1000,'oz':29.5735,'fl oz':29.5735}\n",
        "        return float(val) * unit_map[unit]\n",
        "    except:\n",
        "        return np.nan\n",
        "\n",
        "df_source['source_ml'] = df_source['SourceSize'].apply(parse_size)\n",
        "df_master['master_ml'] = df_master['ProductSizeOnLabel'].apply(parse_size)\n",
        "\n",
        "# 14. Unique barcode lookup\n",
        "df_valid = df_source[df_source.IsValidBarcode==1]\n",
        "bc_counts = df_valid.groupby('CleanBarcode')['ProductMasterId'].nunique()\n",
        "unique_bcs = set(bc_counts[bc_counts==1].index)\n",
        "barcode_dict = {bc:pid for pid,bc in zip(df_master.index, df_master['CleanBarcode']) if bc in unique_bcs}\n",
        "\n",
        "# 15. Split data\n",
        "dev, test = train_test_split(df_source, test_size=0.2, random_state=42, stratify=df_source['ProductMasterId'])\n",
        "print(f\"Split into Train: {len(dev)}, Test: {len(test)} rows.\")\n",
        "\n",
        "# 16. Feature extractor\n",
        "def make_feature(src, pid):\n",
        "    m = df_master.loc[pid]\n",
        "    feats = []\n",
        "    feats.append(int(src.IsValidBarcode==1 and src.CleanBarcode==m.CleanBarcode))\n",
        "    feats.append(int(src.SourceCategory==m.CategoryName))\n",
        "    s, mm = src.source_ml, m.master_ml\n",
        "    feats.append((min(s,mm)/max(s,mm)) if pd.notna(s) and pd.notna(mm) else 0)\n",
        "    feats.append(fuzz.token_set_ratio(src.query_text, ' '.join([m.BrandName, m.ProductName]))/100)\n",
        "    return feats\n",
        "\n",
        "# 17. Train reranker\n",
        "t_pairs = []\n",
        "for _, src in dev.iterrows():\n",
        "    t_pairs.append((src, src.ProductMasterId, 1))\n",
        "    cnt = 0\n",
        "    for j in I[src.name][:5]:\n",
        "        pid = kb_ids[j]\n",
        "        if pid != src.ProductMasterId:\n",
        "            t_pairs.append((src, pid, 0)); cnt += 1\n",
        "            if cnt >= 3: break\n",
        "X_train = np.array([make_feature(s,p) for s,p,_ in t_pairs])\n",
        "y_train = np.array([l for _,_,l in t_pairs])\n",
        "clf = CalibratedClassifierCV(LogisticRegression(max_iter=1000), cv=3)\n",
        "clf.fit(X_train, y_train)\n",
        "print(\"Reranker trained.\")\n",
        "\n",
        "# 18. Predict & evaluate\n",
        "results = []\n",
        "for i, src in test.iterrows():\n",
        "    if src.IsValidBarcode==1 and src.CleanBarcode in barcode_dict:\n",
        "        ids, confs = [barcode_dict[src.CleanBarcode]], [1.0]\n",
        "    else:\n",
        "        ids, confs = [], []\n",
        "        for j in I[i][:5]:\n",
        "            pid = kb_ids[j]\n",
        "            prob = clf.predict_proba([make_feature(src, pid)])[0,1]\n",
        "            ids.append(pid); confs.append(prob)\n",
        "    actual = src.ProductMasterId\n",
        "    is_correct = actual in ids\n",
        "    pred_str = '|'.join(map(str, ids))\n",
        "    conf_str = '|'.join(f\"{c:.2f}\" for c in confs)\n",
        "    results.append((src.ServiceAndProductMappingId, pred_str, conf_str, actual, is_correct))\n",
        "\n",
        "res_df = pd.DataFrame(results, columns=['ID','Pred','Conf','Actual','Correct'])\n",
        "\n",
        "# 19. Metrics and export\n",
        "overall_hit = res_df['Correct'].mean()\n",
        "single = ~res_df['Pred'].str.contains('|', regex=False)\n",
        "acc_single = res_df[single]['Correct'].mean()\n",
        "acc_multi = res_df[~single]['Correct'].mean()\n",
        "print(f\"Accuracy@1 (single-ID cases): {acc_single:.2%}\")\n",
        "print(f\"Hit@5 (multi-ID cases): {acc_multi:.2%}\")\n",
        "print(f\"Overall Hit@5: {overall_hit:.2%}\")\n",
        "\n",
        "import csv\n",
        "# point this at your mounted Drive folder\n",
        "out_folder = '/content/drive/MyDrive/HackWeekC1/'\n",
        "\n",
        "# build the filename with your metrics\n",
        "filename = out_folder + f\"faiss_top5_results_{overall_hit*100:.2f}pct_{len(res_df)}rows.csv\"\n",
        "\n",
        "# export (ensure you still have `import csv` at the top)\n",
        "res_df.to_csv(filename, index=False, quoting=csv.QUOTE_ALL)\n",
        "print(f\"Results saved to: {filename}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465,
          "referenced_widgets": [
            "25de35766e0b489cbe20b916589c7ede",
            "4405915a3272410e8977bbbe380d0b4f",
            "129eaf1358014eedb3a1b5e3b0e1b465",
            "42c31c5f955b47d8a093c12caedb9715",
            "ee3ef451c7864e89bd455e0f0d408b84",
            "5fa6651bc90f4f8f9ccf2784554b894e",
            "17899bff111549d78be84cb35eb90eb4",
            "5cc910ddab7c4d80859248b853e421d8",
            "456a29db067145ca81dae964cd1b4cb3",
            "157bba78ec85410ca1f5f4210fdca209",
            "eb162139515d4f2da4d20812de1b40dc",
            "8932ed456e6e497796940669add27ca8",
            "dbc2ff6687aa4c4dbe8e243a416750f0",
            "68043de144674dfba0f6eec54111e835",
            "89a8c9dc5f9d4004beb3f9f38738c4c4",
            "07d3efc36d36445b9e25395e4345e8f7",
            "b1ca280ddfdc4689a2ef779ef3b521dc",
            "e661c4a1bc8047e0aff2ec1395146b02",
            "ef13367372494a7b9cac275bd45f3a54",
            "ebfd614bf0ad447092a6ca7984141707",
            "292b3087f3a9434ea1977d819ccc5f6a",
            "84d2ed5d244648d8b2dc6de81a5b909a"
          ]
        },
        "id": "UF65IeWwdQRV",
        "outputId": "8cd5dd69-508e-4a81-eee9-2d639c1ed167"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading full data from /content/drive/MyDrive/HackWeekC1/HackWeekProductsData.csv...\n",
            "→ Total rows read: 4165840\n",
            "Filtered to 2996970 mapped rows (dropped 1168870).\n",
            "Found 12249 SKUs with >= 50 examples.\n",
            "Selecting top 12000 SKUs (max cap 12000) => target rows 600000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-819996495.py:52: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
            "  .apply(lambda g: g.sample(n=min_examples, random_state=42))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sampled dataframe: 600000 rows across 12000 SKUs.\n",
            "Master SKUs in index: 12000\n",
            "Embedding KB...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/375 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "25de35766e0b489cbe20b916589c7ede"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding queries...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/18750 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8932ed456e6e497796940669add27ca8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FAISS index built: 12000 entries, dim=384\n",
            "Retrieved top-20 candidates for 600000 queries.\n",
            "Split into Train: 480000, Test: 120000 rows.\n",
            "Reranker trained.\n",
            "Accuracy@1 (single-ID cases): 99.81%\n",
            "Hit@5 (multi-ID cases): 48.17%\n",
            "Overall Hit@5: 55.09%\n",
            "Results saved to: /content/drive/MyDrive/HackWeekC1/faiss_top5_results_55.09pct_120000rows.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Identify Mapped SKUs\n",
        "Reload the full dataset to see what percentage of all SKUs are covered by our training subset.\n"
      ],
      "metadata": {
        "id": "2R_3fK8-ZSgm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# 1. Load full CSV (mapped rows only)\n",
        "path = '/content/drive/MyDrive/HackWeekC1/HackWeekProductsData.csv'\n",
        "df = pd.read_csv(path, encoding='utf-8-sig')\n",
        "if 'MappingModeName' in df.columns:\n",
        "    df = df[df['MappingModeName'].str.strip().str.lower()=='map']\n",
        "\n",
        "# 2. Count SKUs and total mapped rows\n",
        "total_mapped = len(df)\n",
        "sku_counts = df['ProductMasterId'].value_counts()\n",
        "\n",
        "# 3. Identify SKUs with ≥50 examples\n",
        "skus_50 = sku_counts[ sku_counts >= 50 ].index\n",
        "rows_50 = df[df['ProductMasterId'].isin(skus_50)].shape[0]\n",
        "\n",
        "print(f\"Total mapped rows: {total_mapped}\")\n",
        "print(f\"Distinct SKUs overall: {len(sku_counts)}\")\n",
        "print(f\"SKUs with ≥50 examples: {len(skus_50)}\")\n",
        "print(f\"Rows for SKUs ≥50 examples: {rows_50} ({rows_50/total_mapped:.2%} of mapped)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iQVyhvoNJVRz",
        "outputId": "9bb1599c-0343-4020-b6bd-76b48fa8c750"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total mapped rows: 2996970\n",
            "Distinct SKUs overall: 95010\n",
            "SKUs with ≥50 examples: 12249\n",
            "Rows for SKUs ≥50 examples: 2312153 (77.15% of mapped)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Load Test Results and Analyze False Positives\n",
        "Analyze false positive counts at different confidence thresholds (≥0.00 to ≥0.95) and observe their trends.\n"
      ],
      "metadata": {
        "id": "RApCUxgAZZWe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import csv\n",
        "\n",
        "# 1. Load your Top-5 results CSV\n",
        "results_path = '/content/drive/MyDrive/HackWeekC1/faiss_top5_results_55.09pct_120000rows.csv'\n",
        "res = pd.read_csv(results_path, quoting=csv.QUOTE_ALL)\n",
        "\n",
        "# 2. Keep only the multi-ID (pipe-separated) cases\n",
        "mask_multi = res['Pred'].str.contains('|', regex=False)\n",
        "df_multi = res[mask_multi].copy()\n",
        "\n",
        "# 3. Extract max confidence from each pipe-separated string\n",
        "df_multi['max_conf'] = (\n",
        "    df_multi['Conf']\n",
        "      .str.split('|')\n",
        "      .apply(lambda parts: max(float(p) for p in parts))\n",
        ")\n",
        "\n",
        "# 4. Overall false positives\n",
        "total_multi = len(df_multi)\n",
        "false_total = df_multi[~df_multi['Correct']].shape[0]\n",
        "print(f\"Multi-ID rows: {total_multi}, False-positives: {false_total}\")\n",
        "\n",
        "# 5. False-positive counts & rates at thresholds 0.50, 0.60, 0.70, 0.90\n",
        "for thr in [0.00,0.10,0.20,0.30,0.40,0.50, 0.60, 0.70, 0.80, 0.90, 0.95]:\n",
        "    sub = df_multi[df_multi['max_conf'] >= thr]\n",
        "    fp = sub[~sub['Correct']].shape[0]\n",
        "    print(f\"Threshold ≥{thr:.2f}: {len(sub)} rows → {fp} false-positives ({fp/len(sub):.2%})\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8jkuNYIHJp8U",
        "outputId": "4e68a67d-a39b-4245-acab-df71fa599510"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Multi-ID rows: 103907, False-positives: 53859\n",
            "Threshold ≥0.00: 103907 rows → 53859 false-positives (51.83%)\n",
            "Threshold ≥0.10: 103907 rows → 53859 false-positives (51.83%)\n",
            "Threshold ≥0.20: 49548 rows → 21892 false-positives (44.18%)\n",
            "Threshold ≥0.30: 18439 rows → 3202 false-positives (17.37%)\n",
            "Threshold ≥0.40: 10537 rows → 935 false-positives (8.87%)\n",
            "Threshold ≥0.50: 9916 rows → 791 false-positives (7.98%)\n",
            "Threshold ≥0.60: 9758 rows → 781 false-positives (8.00%)\n",
            "Threshold ≥0.70: 9758 rows → 781 false-positives (8.00%)\n",
            "Threshold ≥0.80: 9758 rows → 781 false-positives (8.00%)\n",
            "Threshold ≥0.90: 8839 rows → 686 false-positives (7.76%)\n",
            "Threshold ≥0.95: 2523 rows → 170 false-positives (6.74%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Compute Accuracy at Thresholds (Excluding Conf = 1.0)\n",
        "Load results and compute overall accuracy at different thresholds (ignoring rows with confidence == 1.0).\n"
      ],
      "metadata": {
        "id": "gKHype1jZhNC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import csv\n",
        "\n",
        "# 1) Load your saved Top-5 results CSV\n",
        "res_df = pd.read_csv(\n",
        "    '/content/drive/MyDrive/HackWeekC1/faiss_top5_results_55.09pct_120000rows.csv',\n",
        "    quoting=csv.QUOTE_ALL\n",
        ")\n",
        "\n",
        "print(\"Columns in results:\", res_df.columns.tolist())\n",
        "# Make sure you see a column named exactly 'Correct'.\n",
        "\n",
        "# 2) Split into single-ID vs multi-ID\n",
        "single_mask = ~res_df['Pred'].str.contains('|', regex=False)\n",
        "multi_mask  = ~single_mask\n",
        "\n",
        "# 3) For multi-ID, compute max_conf across the pipe-separated Conf string\n",
        "res_df.loc[multi_mask, 'max_conf'] = (\n",
        "    res_df.loc[multi_mask, 'Conf']\n",
        "          .str.split('|')\n",
        "          .apply(lambda parts: max(float(p) for p in parts))\n",
        ")\n",
        "\n",
        "# 4) Now loop thresholds including 1.0\n",
        "for thr in [1.0, 0.95, 0.90, 0.80, 0.70, 0.60, 0.50]:\n",
        "    if thr == 1.0:\n",
        "        sub = res_df[single_mask]\n",
        "    else:\n",
        "        sub = res_df[multi_mask & (res_df['max_conf'] >= thr)]\n",
        "    n  = len(sub)\n",
        "    fp = sub[~sub['Correct']].shape[0]\n",
        "    acc = sub['Correct'].mean() if n else float('nan')\n",
        "    print(f\"Thr ≥{thr:.2f}: {n:6d} rows → {fp:4d} false-positives,  Accuracy: {acc:.2%}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k7KwqSIoLF7o",
        "outputId": "0e86caf7-37b5-45a8-d5fc-c5d1b33c366a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns in results: ['ID', 'Pred', 'Conf', 'Actual', 'Correct']\n",
            "Thr ≥1.00:  16093 rows →   30 false-positives,  Accuracy: 99.81%\n",
            "Thr ≥0.95:   2523 rows →  170 false-positives,  Accuracy: 93.26%\n",
            "Thr ≥0.90:   8839 rows →  686 false-positives,  Accuracy: 92.24%\n",
            "Thr ≥0.80:   9758 rows →  781 false-positives,  Accuracy: 92.00%\n",
            "Thr ≥0.70:   9758 rows →  781 false-positives,  Accuracy: 92.00%\n",
            "Thr ≥0.60:   9758 rows →  781 false-positives,  Accuracy: 92.00%\n",
            "Thr ≥0.50:   9916 rows →  791 false-positives,  Accuracy: 92.02%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 7. Compute Accuracy Including Confidence == 1.0\n",
        "Update analysis to include rows with Conf = 1.0 to get complete accuracy picture."
      ],
      "metadata": {
        "id": "eN8Bz2zTZlhi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import csv\n",
        "\n",
        "# 1) Load your Top-5 results CSV from Drive\n",
        "results_path = '/content/drive/MyDrive/HackWeekC1/faiss_top5_results_55.09pct_120000rows.csv'\n",
        "res_df = pd.read_csv(results_path, quoting=csv.QUOTE_ALL, encoding='utf-8-sig')\n",
        "\n",
        "# 2) Inspect what columns you have\n",
        "print(\"Columns in results:\", res_df.columns.tolist())\n",
        "\n",
        "# Expect at least: ['ID','Pred','Conf','Actual','Correct']\n",
        "\n",
        "# 3) Compute a unified best_conf per row (works for single or pipe-separated values)\n",
        "def get_best_conf(cell):\n",
        "    parts = str(cell).split('|')       # even \"1.00\" → [\"1.00\"]\n",
        "    return max(float(p) for p in parts)\n",
        "\n",
        "res_df['best_conf'] = res_df['Conf'].apply(get_best_conf)\n",
        "\n",
        "# 4) Evaluate false-positives & accuracy at multiple thresholds\n",
        "thresholds = [1.00, 0.95, 0.90, 0.80, 0.70, 0.60, 0.50,0.40,0.30,0.20,0.10,0.05,0.00]\n",
        "for thr in thresholds:\n",
        "    sub = res_df[ res_df['best_conf'] >= thr ]\n",
        "    n  = len(sub)\n",
        "    fp = sub[~sub['Correct']].shape[0]\n",
        "    acc = sub['Correct'].mean() if n>0 else float('nan')\n",
        "    print(f\"Threshold ≥{thr:.2f}: {n:6d} rows → {fp:4d} false-positives,  Accuracy: {acc:.2%}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j2f99zGYOJzG",
        "outputId": "443c8f29-3451-4160-be0c-6955e2cbf5cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns in results: ['ID', 'Pred', 'Conf', 'Actual', 'Correct']\n",
            "Threshold ≥1.00:  16093 rows →   30 false-positives,  Accuracy: 99.81%\n",
            "Threshold ≥0.95:  18616 rows →  200 false-positives,  Accuracy: 98.93%\n",
            "Threshold ≥0.90:  24932 rows →  716 false-positives,  Accuracy: 97.13%\n",
            "Threshold ≥0.80:  25851 rows →  811 false-positives,  Accuracy: 96.86%\n",
            "Threshold ≥0.70:  25851 rows →  811 false-positives,  Accuracy: 96.86%\n",
            "Threshold ≥0.60:  25851 rows →  811 false-positives,  Accuracy: 96.86%\n",
            "Threshold ≥0.50:  26009 rows →  821 false-positives,  Accuracy: 96.84%\n",
            "Threshold ≥0.40:  26630 rows →  965 false-positives,  Accuracy: 96.38%\n",
            "Threshold ≥0.30:  34532 rows → 3232 false-positives,  Accuracy: 90.64%\n",
            "Threshold ≥0.20:  65641 rows → 21922 false-positives,  Accuracy: 66.60%\n",
            "Threshold ≥0.10: 120000 rows → 53889 false-positives,  Accuracy: 55.09%\n",
            "Threshold ≥0.05: 120000 rows → 53889 false-positives,  Accuracy: 55.09%\n",
            "Threshold ≥0.00: 120000 rows → 53889 false-positives,  Accuracy: 55.09%\n"
          ]
        }
      ]
    }
  ]
}
